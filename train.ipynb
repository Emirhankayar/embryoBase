{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91647e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Install the required libraries\\n#SAM\\n%pip install git+https://github.com/facebookresearch/segment-anything.git\\n#Transformers\\n%pip install -q git+https://github.com/huggingface/transformers.git\\n#Datasets to prepare data and monai if you want to use special loss functions\\n%pip install datasets\\n%pip install -q monai\\n#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)\\n%pip install patchify'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Install the required libraries\n",
    "#SAM\n",
    "%pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "#Transformers\n",
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "#Datasets to prepare data and monai if you want to use special loss functions\n",
    "%pip install datasets\n",
    "%pip install -q monai\n",
    "#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)\n",
    "%pip install patchify\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d81d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9db9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"/home/phd2/Scrivania/CorsoData/blastocisti\"\n",
    "csv = \"bbox_final.csv\"\n",
    "save_dir = \"trained_model\"\n",
    "\n",
    "# Index available .h5 files by stem (case-insensitive)\n",
    "h5_index = {}\n",
    "for root, dirs, files in os.walk(src, topdown=True):\n",
    "    for f in files:\n",
    "        fl = f.lower()\n",
    "        if fl.endswith((\".h5\", \".hdf5\")):\n",
    "            stem = os.path.splitext(fl)[0]\n",
    "            h5_index[stem] = os.path.join(root, f)\n",
    "\n",
    "# Load CSV rows (keep only basename of Image column)\n",
    "df = (\n",
    "    pl.scan_csv(csv)\n",
    "    .select([\"Image\", \"x1\", \"y1\", \"x2\", \"y2\", \"Label\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"Image\").str.replace_all(r\".*[\\\\/]\", \"\").alias(\"Image\")\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "rows = df.to_dicts()\n",
    "\n",
    "# Match CSV .jpg to on-disk .h5 by stem\n",
    "valid_rows = []\n",
    "not_found = 0\n",
    "for r in rows:\n",
    "    name = r[\"Image\"]\n",
    "    stem = os.path.splitext(name)[0].lower()  # strip extension (.jpg) and lowercase\n",
    "    h5_path = h5_index.get(stem)\n",
    "    if h5_path and os.path.exists(h5_path):\n",
    "        r[\"FullImagePath\"] = h5_path\n",
    "        valid_rows.append(r)\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "# Use ALL matched rows (no subsampling), then split\n",
    "if len(valid_rows) == 0:\n",
    "    print(\"Warning: 0 rows matched to .h5 files. Ensure JPGs were converted to HDF5 and 'src' is correct.\")\n",
    "\n",
    "labels_list = [r[\"Label\"] for r in valid_rows]\n",
    "from collections import Counter\n",
    "counts = Counter(labels_list)\n",
    "can_stratify = all(c >= 2 for c in counts.values()) and len(set(labels_list)) > 1\n",
    "\n",
    "train_rows, test_rows = sk.model_selection.train_test_split(\n",
    "    valid_rows,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_list if can_stratify else None,\n",
    ")\n",
    "# print(f\"Matched {len(valid_rows)} of {len(rows)} CSV entries; Train: {len(train_rows)} Test: {len(test_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3548d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py  # add this import\n",
    "import cv2\n",
    "\n",
    "def _find_first_dataset(h5obj):\n",
    "    # Recursively find the first dataset in the file/group\n",
    "    for k in h5obj.keys():\n",
    "        v = h5obj[k]\n",
    "        if isinstance(v, h5py.Dataset):\n",
    "            return v\n",
    "        if isinstance(v, h5py.Group):\n",
    "            d = _find_first_dataset(v)\n",
    "            if d is not None:\n",
    "                return d\n",
    "    return None\n",
    "\n",
    "def load_images_and_masks(rows, target_size=(1024, 1024), method='as_is'):\n",
    "    \"\"\"\n",
    "    Load images from .h5 per item, resize to target_size, and generate circular masks.\n",
    "    Args:\n",
    "        rows: List of dicts with FullImagePath, x1, y1, x2, y2, Label\n",
    "        target_size: (W, H)\n",
    "        method: 'as_is' (bbox already in resized space) or 'scale' (bbox from original image)\n",
    "    \"\"\"\n",
    "    images, masks, labels = [], [], []\n",
    "\n",
    "    for r in rows:\n",
    "        img_path = r.get(\"FullImagePath\")\n",
    "        if not img_path or not os.path.exists(img_path):\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Open HDF5 and read image array for this item only\n",
    "            with h5py.File(img_path, \"r\") as f:\n",
    "                ds = f[\"image\"] if \"image\" in f else _find_first_dataset(f)\n",
    "                if ds is None:\n",
    "                    print(f\"No dataset found in: {img_path}\")\n",
    "                    continue\n",
    "                arr = ds[...]  # read this image now\n",
    "\n",
    "            # Normalize shape to HxWxC\n",
    "            if arr.ndim == 2:\n",
    "                orig_h, orig_w = arr.shape\n",
    "                img_np = np.stack([arr] * 3, axis=-1)\n",
    "            elif arr.ndim == 3:\n",
    "                # If channels-first (C,H,W) convert to (H,W,C)\n",
    "                if arr.shape[0] in (1, 3) and arr.shape[-1] not in (1, 3):\n",
    "                    orig_h, orig_w = arr.shape[1], arr.shape[2]\n",
    "                    img_np = np.transpose(arr, (1, 2, 0))\n",
    "                else:\n",
    "                    orig_h, orig_w = arr.shape[0], arr.shape[1]\n",
    "                    img_np = arr\n",
    "            else:\n",
    "                print(f\"Unsupported array shape {arr.shape} in {img_path}\")\n",
    "                continue\n",
    "\n",
    "            # Convert to uint8 for resizing if needed\n",
    "            if img_np.dtype != np.uint8:\n",
    "                vmin, vmax = float(img_np.min()), float(img_np.max())\n",
    "                if vmax > vmin:\n",
    "                    img_np = ((img_np - vmin) / (vmax - vmin) * 255.0).astype(np.uint8)\n",
    "                else:\n",
    "                    img_np = np.zeros_like(img_np, dtype=np.uint8)\n",
    "\n",
    "            # Resize to target_size\n",
    "            img_np = cv2.resize(img_np, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "            h, w = img_np.shape[:2]\n",
    "\n",
    "            # BBox handling\n",
    "            if method == 'as_is':\n",
    "                x1 = int(np.clip(r[\"x1\"], 0, w - 1))\n",
    "                y1 = int(np.clip(r[\"y1\"], 0, h - 1))\n",
    "                x2 = int(np.clip(r[\"x2\"], 0, w - 1))\n",
    "                y2 = int(np.clip(r[\"y2\"], 0, h - 1))\n",
    "            elif method == 'scale':\n",
    "                scale_x = w / max(1, orig_w)\n",
    "                scale_y = h / max(1, orig_h)\n",
    "                x1 = int(np.clip(r[\"x1\"] * scale_x, 0, w - 1))\n",
    "                y1 = int(np.clip(r[\"y1\"] * scale_y, 0, h - 1))\n",
    "                x2 = int(np.clip(r[\"x2\"] * scale_x, 0, w - 1))\n",
    "                y2 = int(np.clip(r[\"y2\"] * scale_y, 0, h - 1))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            # Circular mask from bbox\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            radius = int(min((x2 - x1), (y2 - y1)) / 2)\n",
    "            if radius <= 0:\n",
    "                continue\n",
    "\n",
    "            mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
    "\n",
    "            if np.count_nonzero(mask) == 0:\n",
    "                continue\n",
    "\n",
    "            images.append(img_np)\n",
    "            masks.append(mask)\n",
    "            labels.append(r[\"Label\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Loaded {len(images)} valid images @ {target_size}\")\n",
    "    return images, masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506d51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lazy HDF5 pipeline. Train rows: 2529536, Test rows: 632384\n"
     ]
    }
   ],
   "source": [
    "# --- Skip eager loading to avoid high RAM ---\n",
    "print(f\"Using lazy HDF5 pipeline. Train rows: {len(train_rows)}, Test rows: {len(test_rows)}\")\n",
    "# Images and masks will be read on-the-fly by the Dataset during training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c17819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined H5LazyEmbryoDataset (robust shape handling + bbox rectangle fallback)\n"
     ]
    }
   ],
   "source": [
    "# Build a lazy HDF5-backed Dataset to avoid high RAM\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "class H5LazyEmbryoDataset(Dataset):\n",
    "    def __init__(self, rows, image_size=(1024, 1024), normalize=True):\n",
    "        self.rows = rows\n",
    "        self.image_size = tuple(image_size)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def _read_h5_image(self, h5_path: str) -> np.ndarray:\n",
    "        # Opens HDF5, reads dataset lazily, closes file immediately\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            if 'image' in f:\n",
    "                arr = f['image'][:]\n",
    "            else:\n",
    "                ds_name = next((k for k, v in f.items() if isinstance(v, h5py.Dataset)), None)\n",
    "                if ds_name is None:\n",
    "                    raise ValueError(f\"No dataset found in {h5_path}\")\n",
    "                arr = f[ds_name][:]\n",
    "        return arr\n",
    "\n",
    "    def _to_hwc(self, img: np.ndarray) -> np.ndarray:\n",
    "        img = np.asarray(img)\n",
    "        # Remove all singleton dimensions\n",
    "        img = np.squeeze(img)\n",
    "        if img.ndim == 1:\n",
    "            raise ValueError(f\"Unexpected 1D image shape: {img.shape}\")\n",
    "        if img.ndim == 2:\n",
    "            img = img[..., None]\n",
    "            return img\n",
    "        # If more than 3 dims, iteratively squeeze or slice the first dim\n",
    "        while img.ndim > 3:\n",
    "            # Prefer squeezing singleton dims\n",
    "            if 1 in img.shape:\n",
    "                img = np.squeeze(img)\n",
    "            else:\n",
    "                # Take the first index along the leading dim to reduce dimensionality\n",
    "                img = img.take(indices=0, axis=0)\n",
    "        # Now ensure channels-last\n",
    "        if img.ndim == 3:\n",
    "            # If last dim looks like channels, keep as-is\n",
    "            if img.shape[-1] in (1, 3, 4) and img.shape[0] > 4 and img.shape[1] > 4:\n",
    "                return img\n",
    "            # If first dim looks like channels, move to last\n",
    "            if img.shape[0] in (1, 3, 4) and img.shape[1] > 4 and img.shape[2] > 4:\n",
    "                return np.transpose(img, (1, 2, 0))\n",
    "            # If ambiguous, fall back to treating as single-channel by averaging last axis\n",
    "            return np.mean(img, axis=-1, keepdims=True)\n",
    "        return img\n",
    "\n",
    "    def _resize_image(self, img: np.ndarray) -> np.ndarray:\n",
    "        h, w = img.shape[:2]\n",
    "        if (w, h) == self.image_size:\n",
    "            return img\n",
    "        # Resize depending on channels\n",
    "        if img.shape[2] == 1:\n",
    "            resized = cv2.resize(img[..., 0], self.image_size, interpolation=cv2.INTER_AREA)\n",
    "            return resized[..., None]\n",
    "        else:\n",
    "            return cv2.resize(img, self.image_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    def _normalize_image(self, img: np.ndarray) -> np.ndarray:\n",
    "        img = img.astype(np.float32, copy=False)\n",
    "        if not self.normalize:\n",
    "            return img\n",
    "        if img.dtype == np.uint8:\n",
    "            return img / 255.0\n",
    "        if img.dtype == np.uint16:\n",
    "            return img / 65535.0\n",
    "        maxv = float(img.max()) if img.size else 1.0\n",
    "        if maxv > 1.0:\n",
    "            return img / maxv\n",
    "        return img\n",
    "\n",
    "    def _make_mask(self, row, mask_shape):\n",
    "        h, w = mask_shape[:2]\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        # Try circular mask if center/radius present\n",
    "        def get_first(*names):\n",
    "            for n in names:\n",
    "                if n in row and row[n] is not None:\n",
    "                    return row[n]\n",
    "            return None\n",
    "        cx = get_first('cx', 'center_x', 'x_center', 'x')\n",
    "        cy = get_first('cy', 'center_y', 'y_center', 'y')\n",
    "        r = get_first('r', 'radius')\n",
    "        drew = False\n",
    "        try:\n",
    "            if cx is not None and cy is not None and r is not None:\n",
    "                cv2.circle(mask, (int(round(float(cx))), int(round(float(cy)))), int(round(float(r))), 1, -1)\n",
    "                drew = True\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback: rectangular mask from CSV bbox if available\n",
    "        if not drew:\n",
    "            for k in ('x1','y1','x2','y2'):\n",
    "                if k not in row:\n",
    "                    break\n",
    "            else:\n",
    "                try:\n",
    "                    x1 = int(max(0, min(w-1, float(row['x1']))))\n",
    "                    y1 = int(max(0, min(h-1, float(row['y1']))))\n",
    "                    x2 = int(max(0, min(w-1, float(row['x2']))))\n",
    "                    y2 = int(max(0, min(h-1, float(row['y2']))))\n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        mask[y1:y2, x1:x2] = 1\n",
    "                        drew = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        h5_path = str(row['FullImagePath']) if isinstance(row, dict) else str(row[\"FullImagePath\"])\n",
    "        img = self._read_h5_image(h5_path)\n",
    "        img = self._to_hwc(img)\n",
    "        img = self._resize_image(img)\n",
    "        img = self._normalize_image(img)\n",
    "\n",
    "        mask = self._make_mask(row, img.shape)\n",
    "\n",
    "        # Convert to tensors (C,H,W)\n",
    "        img_t = torch.from_numpy(np.transpose(img, (2, 0, 1)))\n",
    "        mask_t = torch.from_numpy(mask[None, ...].astype(np.float32))\n",
    "\n",
    "        return {\n",
    "            'image': img_t,\n",
    "            'mask': mask_t,\n",
    "            'path': h5_path,\n",
    "        }\n",
    "\n",
    "print(\"Defined H5LazyEmbryoDataset (robust shape handling + bbox rectangle fallback)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4758c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': torch.Size([2, 1, 1024, 1024]), 'mask': torch.Size([2, 1, 1024, 1024]), 'path': <class 'list'>}\n"
     ]
    }
   ],
   "source": [
    "# Wire DataLoaders with low RAM footprint\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_size = (1024, 1024)\n",
    "num_workers = 0  # Avoid Polars + fork issues; increase later with spawn context if needed\n",
    "\n",
    "train_dataset = H5LazyEmbryoDataset(train_rows, image_size=image_size)\n",
    "test_dataset = H5LazyEmbryoDataset(test_rows, image_size=image_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# Quick smoke test (fetch one batch)\n",
    "for batch in train_loader:\n",
    "    print({k: (v.shape if hasattr(v, 'shape') else type(v)) for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61102c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phd2/Scrivania/CorsoVenvs/BlastoVenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-09 16:31:15.542397: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-09 16:31:15.548278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760020275.556170 1380476 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760020275.558542 1380476 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760020275.565033 1380476 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760020275.565042 1380476 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760020275.565043 1380476 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760020275.565043 1380476 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-09 16:31:15.567400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined SAMDatasetLazy and initialized processor (moved earlier for correct execution order)\n"
     ]
    }
   ],
   "source": [
    "# Define SAM processor and dataset wrapper BEFORE building sam_* datasets\n",
    "from typing import Dict, Any\n",
    "from PIL import Image\n",
    "from transformers import SamProcessor\n",
    "\n",
    "# Initialize the processor once (idempotent)\n",
    "try:\n",
    "    processor\n",
    "except NameError:\n",
    "    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "class SAMDatasetLazy(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset: H5LazyEmbryoDataset, processor, bbox_keys=(\"x1\",\"y1\",\"x2\",\"y2\")):\n",
    "        self.base = base_dataset\n",
    "        self.rows = base_dataset.rows\n",
    "        self.processor = processor\n",
    "        self.bx, self.by, self.bX, self.bY = bbox_keys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def _get_bbox(self, row, w, h):\n",
    "        # Prefer explicit bbox if available\n",
    "        if all(k in row and row[k] is not None for k in (self.bx,self.by,self.bX,self.bY)):\n",
    "            x1 = int(max(0, min(w-1, float(row[self.bx]))))\n",
    "            y1 = int(max(0, min(h-1, float(row[self.by]))))\n",
    "            x2 = int(max(0, min(w-1, float(row[self.bX]))))\n",
    "            y2 = int(max(0, min(h-1, float(row[self.bY]))))\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                return [x1, y1, x2, y2]\n",
    "        # Fallback: derive from circular mask center/radius if present\n",
    "        cx = row.get('cx'); cy = row.get('cy'); r = row.get('r')\n",
    "        if cx is not None and cy is not None and r is not None:\n",
    "            cx, cy, r = float(cx), float(cy), float(r)\n",
    "            x1 = int(max(0, min(w-1, cx - r)))\n",
    "            y1 = int(max(0, min(h-1, cy - r)))\n",
    "            x2 = int(max(0, min(w-1, cx + r)))\n",
    "            y2 = int(max(0, min(h-1, cy + r)))\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                return [x1, y1, x2, y2]\n",
    "        # Last resort: center box covering middle of the image\n",
    "        bw = int(w*0.3); bh = int(h*0.3)\n",
    "        x1 = (w - bw)//2; y1 = (h - bh)//2\n",
    "        x2 = x1 + bw; y2 = y1 + bh\n",
    "        return [x1, y1, x2, y2]\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, Any]:\n",
    "        sample = self.base[idx]\n",
    "        row = self.rows[idx]\n",
    "        # Convert CxHxW float[0,1] -> HxWxC uint8 RGB\n",
    "        img_t = sample['image']  # C,H,W\n",
    "        if img_t.ndim != 3:\n",
    "            raise ValueError(f\"Unexpected image tensor shape: {tuple(img_t.shape)}\")\n",
    "        c, h, w = img_t.shape\n",
    "        np_img = (img_t.permute(1,2,0).numpy() * 255.0).clip(0,255).astype(np.uint8)\n",
    "        if c == 1:\n",
    "            np_img = np.repeat(np_img, 3, axis=2)\n",
    "        elif c > 3:\n",
    "            np_img = np_img[:, :, :3]\n",
    "        bbox = self._get_bbox(row, w, h)\n",
    "        # Prepare inputs for SAM\n",
    "        inputs = self.processor(np_img, input_boxes=[[bbox]], return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k,v in inputs.items()}  # remove batch dim\n",
    "        # Attach ground truth mask (HxW -> float32)\n",
    "        gt = sample['mask'].squeeze(0).float()  # HxW\n",
    "        inputs['ground_truth_mask'] = gt\n",
    "        return inputs\n",
    "\n",
    "print(\"Defined SAMDatasetLazy and initialized processor (moved earlier for correct execution order)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea3ee2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset of 500 samples (from 2529536)\n",
      "{'pixel_values': torch.Size([2, 3, 1024, 1024]), 'original_sizes': torch.Size([2, 2]), 'reshaped_input_sizes': torch.Size([2, 2]), 'input_boxes': torch.Size([2, 1, 4]), 'ground_truth_mask': torch.Size([2, 1024, 1024])}\n"
     ]
    }
   ],
   "source": [
    "# Build the SAM-ready lazy dataset and DataLoader (use a subset for speed)\n",
    "sam_train = SAMDatasetLazy(train_dataset, processor)\n",
    "sam_test = SAMDatasetLazy(test_dataset, processor)\n",
    "\n",
    "# Subset to at most 500 samples to speed up validation runs\n",
    "# subset_size = min(500, len(sam_train))\n",
    "subset_size = min(500,len(sam_train))\n",
    "import random\n",
    "random.seed(42)\n",
    "indices = list(range(len(sam_train)))\n",
    "random.shuffle(indices)\n",
    "subset_indices = indices[:subset_size]\n",
    "\n",
    "sam_train_subset = torch.utils.data.Subset(sam_train, subset_indices)\n",
    "\n",
    "sam_train_loader = DataLoader(sam_train_subset, batch_size=2, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Using subset of {len(sam_train_subset)} samples (from {len(sam_train)})\")\n",
    "\n",
    "# Smoke test: get a batch compatible with SamModel\n",
    "batch = next(iter(sam_train_loader))\n",
    "print({k: v.shape if hasattr(v, 'shape') else type(v) for k,v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff49f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: integrate with SAM processor/model lazily later.\n",
    "# For now, this keeps memory stable by not preloading all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31df9013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMDatasetLazy already defined above; skipping redefinition here.\n"
     ]
    }
   ],
   "source": [
    "# Wrap the lazy dataset to feed SAM: pixel_values, input_boxes, ground_truth_mask\n",
    "# (Defined earlier to ensure order). Keeping this cell as a note to avoid duplicate class definitions.\n",
    "print(\"SAMDatasetLazy already defined above; skipping redefinition here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4281ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm((4,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_norm2): SamLayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "from transformers import SamModel\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Freeze encoder components; train only mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0455086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import monai\n",
    "# Initialize the optimizer and the loss function\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fa699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/250 [00:00<01:50,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred_masks_shape': (2, 1, 256, 256), 'gt_masks_shape': (2, 1, 256, 256)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:53<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 | mean loss: 0.1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 226/250 [00:48<00:05,  4.69it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "# Training loop (lazy loaders -> stable RAM). Limit to subset for quick validation.\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "debug_once = True\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(sam_train_loader):\n",
    "        # Move to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_boxes = batch[\"input_boxes\"].to(device)\n",
    "        gt_masks = batch[\"ground_truth_mask\"].float().to(device)  # [B, H, W]\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(pixel_values=pixel_values, input_boxes=input_boxes, multimask_output=False)\n",
    "        pred_masks = outputs.pred_masks  # could be [B, 1, 1, H, W] or [B, 1, H, W]\n",
    "\n",
    "        # Normalize pred_masks to [B, 1, H, W]\n",
    "        if pred_masks.dim() == 5:\n",
    "            # [B, N, C, H, W] -> take first mask set\n",
    "            pred_masks = pred_masks[:, 0]  # [B, C, H, W]\n",
    "        if pred_masks.dim() == 4 and pred_masks.shape[1] != 1:\n",
    "            # If multiple channels/masks, keep the first channel\n",
    "            pred_masks = pred_masks[:, :1]\n",
    "        if pred_masks.dim() == 3:\n",
    "            pred_masks = pred_masks.unsqueeze(1)\n",
    "        pred_masks = pred_masks.float()\n",
    "\n",
    "        # Resize ground truth to predicted resolution [B,1,H_out,W_out]\n",
    "        if gt_masks.dim() == 3:\n",
    "            gt_masks = gt_masks.unsqueeze(1)\n",
    "        if gt_masks.shape[-2:] != pred_masks.shape[-2:]:\n",
    "            gt_masks = interpolate(gt_masks, size=pred_masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        if debug_once:\n",
    "            print({\n",
    "                'pred_masks_shape': tuple(pred_masks.shape),\n",
    "                'gt_masks_shape': tuple(gt_masks.shape),\n",
    "            })\n",
    "            debug_once = False\n",
    "\n",
    "        loss = seg_loss(pred_masks, gt_masks)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f\"EPOCH: {epoch} | mean loss: {mean(epoch_losses):.4f}\")\n",
    "\n",
    "print(\"Training loop finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary to a file\n",
    "torch.save(model.state_dict(), \"trained_model/mito_model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7e72f",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10eabe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model weights for inference\n",
    "from transformers import SamModel, SamProcessor\n",
    "import torch\n",
    "\n",
    "# Reuse existing processor if available; otherwise load\n",
    "try:\n",
    "    processor\n",
    "except NameError:\n",
    "    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "my_mito_model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "state_dict = torch.load(\"trained_model/mito_model_checkpoint.pth\", map_location=\"cpu\")\n",
    "my_mito_model.load_state_dict(state_dict)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "my_mito_model.to(device)\n",
    "my_mito_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73886365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device to cuda if available, otherwise use cpu (already set above)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "my_mito_model.to(device)\n",
    "my_mito_model.eval()\n",
    "print(f\"Inference device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74499650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick a random sample from the lazy test dataset\n",
    "if len(test_dataset) == 0:\n",
    "    raise RuntimeError(\"No test samples available.\")\n",
    "idx = random.randint(0, len(test_dataset)-1)\n",
    "\n",
    "# Read one sample via lazy dataset\n",
    "base_sample = test_dataset[idx]\n",
    "row = test_rows[idx]\n",
    "# Convert CxHxW [0,1] -> HxWxC uint8 for visualization and processor\n",
    "img_t = base_sample['image']\n",
    "c,h,w = img_t.shape\n",
    "np_img = (img_t.permute(1,2,0).numpy() * 255.0).clip(0,255).astype(np.uint8)\n",
    "if c == 1:\n",
    "    np_img = np.repeat(np_img, 3, axis=2)\n",
    "elif c > 3:\n",
    "    np_img = np_img[:, :, :3]\n",
    "\n",
    "# Ensure target size is 1024x1024 as per your CSV bboxes\n",
    "if (h, w) != (1024, 1024):\n",
    "    print(f\"Warning: image is {h}x{w}, expected 1024x1024. Resizing display copy only; CSV bbox stays in 1024 space.\")\n",
    "    # Create a resized copy just for display & processor; keep original (h,w) for indexing\n",
    "    import cv2  # ensure available in this cell\n",
    "    disp_img = cv2.resize(np_img, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "else:\n",
    "    disp_img = np_img\n",
    "\n",
    "# Use CSV bbox directly (already 1024x1024 space)\n",
    "x1 = int(max(0, min(1023, float(row.get('x1', 0)))))\n",
    "y1 = int(max(0, min(1023, float(row.get('y1', 0)))))\n",
    "x2 = int(max(0, min(1023, float(row.get('x2', 1023)))))\n",
    "y2 = int(max(0, min(1023, float(row.get('y2', 1023)))))\n",
    "if not (x2 > x1 and y2 > y1):\n",
    "    # fallback to center box in 1024 space\n",
    "    bw = int(1024*0.3); bh = int(1024*0.3); x1=(1024-bw)//2; y1=(1024-bh)//2; x2=x1+bw; y2=y1+bh\n",
    "bbox = [x1, y1, x2, y2]\n",
    "\n",
    "# Prepare inputs with the display image at 1024 and CSV bbox\n",
    "inputs = processor(disp_img, input_boxes=[[bbox]], return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "my_mito_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = my_mito_model(**inputs, multimask_output=True)\n",
    "\n",
    "# Pick best mask index robustly\n",
    "best_idx = 0\n",
    "if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "    iou = outputs.iou_scores\n",
    "    if iou.ndim == 2:\n",
    "        iou = iou[0]\n",
    "    best_idx = int(torch.argmax(iou).item())\n",
    "\n",
    "# Map to image space via post-processing\n",
    "post = processor.post_process_masks(\n",
    "    outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n",
    ")\n",
    "# Normalize shapes to get a single 2D mask logit map\n",
    "m = post[0]\n",
    "if not isinstance(m, torch.Tensor):\n",
    "    m = torch.as_tensor(m)\n",
    "# Accept (1,C,H,W) -> (C,H,W), (C,H,W) -> select, (H,W) -> use directly\n",
    "if m.ndim == 4:\n",
    "    m = m[0]\n",
    "if m.ndim == 3:\n",
    "    sel = int(min(max(best_idx, 0), m.shape[0]-1))\n",
    "    mask_logits = m[sel]\n",
    "elif m.ndim == 2:\n",
    "    mask_logits = m\n",
    "else:\n",
    "    mask_logits = m.squeeze()\n",
    "\n",
    "# Convert logits to probability map (HxW)\n",
    "prob_map = torch.sigmoid(mask_logits.float()).cpu().numpy()\n",
    "if prob_map.ndim == 3:\n",
    "    # In rare cases, squeeze any singleton channel dim\n",
    "    prob_map = np.squeeze(prob_map)\n",
    "pred_mask = (prob_map > 0.5).astype(np.uint8)\n",
    "\n",
    "print({\n",
    "    'img_shape': disp_img.shape,\n",
    "    'prob_min': float(prob_map.min()) if prob_map.size else None,\n",
    "    'prob_max': float(prob_map.max()) if prob_map.size else None,\n",
    "    'prob_mean': float(prob_map.mean()) if prob_map.size else None,\n",
    "    'mask_shape': tuple(pred_mask.shape),\n",
    "    'mask_sum': int(pred_mask.sum()),\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axes[0].imshow(disp_img)\n",
    "# Draw bbox\n",
    "axes[0].add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='lime', linewidth=2))\n",
    "axes[0].set_title(\"Image + BBox (1024)\")\n",
    "axes[1].imshow(pred_mask, cmap='gray')\n",
    "axes[1].set_title(\"Mask (thr 0.5)\")\n",
    "# Overlay probability map on image with alpha\n",
    "axes[2].imshow(disp_img)\n",
    "overlay = axes[2].imshow(prob_map, vmin=0.0, vmax=1.0, cmap='magma', alpha=0.5)\n",
    "axes[2].set_title(\"Image + Probability Overlay\")\n",
    "for ax in axes:\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.colorbar(overlay, ax=axes[2], fraction=0.046, pad=0.04, label='Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BlastoVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
